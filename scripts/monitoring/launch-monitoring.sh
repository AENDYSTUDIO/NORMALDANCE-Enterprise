#!/bin/bash# NormalDance Launch and Monitoring Script# This script handles the final launch preparation and ongoing monitoringset -e# ConfigurationNAMESPACE="production"CLUSTER_NAME="normaldance-prod"REGION="us-east-1"MONITORING_DURATION="30d"HEALTH_CHECK_INTERVAL="30s"METRICS_RETENTION="30d"ALERT_EMAILS="admin@normaldance.app,devops@normaldance.app"SLACK_WEBHOOK="${SLACK_WEBHOOK:-}"GRAFANA_URL="${GRAFANA_URL:-}"PROMETHEUS_URL="${PROMETHEUS_URL:-}"DATADOG_API_KEY="${DATADOG_API_KEY:-}"# Colors for outputRED='\033[0;31m'GREEN='\033[0;32m'YELLOW='\033[1;33m'NC='\033[0m' # No Color# Logging functionlog() {    echo -e "${GREEN}[INFO]${NC} $1"}warn() {    echo -e "${YELLOW}[WARN]${NC} $1"}error() {    echo -e "${RED}[ERROR]${NC} $1"    exit 1}# Check prerequisitescheck_prerequisites() {    log "Checking prerequisites..."        # Check if required tools are installed    if ! command -v kubectl &> /dev/null; then        error "kubectl is not installed"    fi        if ! command -v helm &> /dev/null; then        error "helm is not installed"    fi        if ! command -v aws &> /dev/null; then        error "aws-cli is not installed"    fi        if ! command -v curl &> /dev/null; then        error "curl is not installed"    fi        if ! command -v jq &> /dev/null; then        error "jq is not installed"    fi        log "All prerequisites are installed"}# Create monitoring directorycreate_monitoring_directory() {    log "Creating monitoring directory..."        # Create monitoring directory    mkdir -p "monitoring/launch"    mkdir -p "monitoring/alerts"    mkdir -p "monitoring/dashboards"    mkdir -p "monitoring/scripts"    mkdir -p "monitoring/logs"        log "Monitoring directory created"}# Set up monitoring infrastructuresetup_monitoring_infrastructure() {    log "Setting up monitoring infrastructure..."        # Create Prometheus configuration    cat > "monitoring/prometheus-config.yaml" << EOFglobal:  scrape_interval: 15s  evaluation_interval: 15srule_files:  - "alert_rules.yml"alerting:  alertmanagers:    - static_configs:        - targets:          - alertmanager:9093scrape_configs:  - job_name: 'normaldance-api'    static_configs:      - targets: ['normaldance-api:3000']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'normaldance-web'    static_configs:      - targets: ['normaldance-web:3000']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'normaldance-db'    static_configs:      - targets: ['normaldance-db:5432']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'normaldance-redis'    static_configs:      - targets: ['normaldance-redis:6379']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'normaldance-ipfs'    static_configs:      - targets: ['normaldance-ipfs:5001']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'normaldance-solana'    static_configs:      - targets: ['normaldance-solana:8899']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'node-exporter'    static_configs:      - targets: ['node-exporter:9100']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'cadvisor'    static_configs:      - targets: ['cadvisor:8080']    metrics_path: '/metrics'    scrape_interval: 15s      - job_name: 'blackbox'    static_configs:      - targets: ['blackbox-exporter:9115']    metrics_path: '/probe'    scrape_interval: 15s    params:      module: [http_2xx]EOF        # Create alert rules    cat > "monitoring/alert_rules.yml" << EOFgroups:  - name: normaldance-alerts    rules:      - alert: HighErrorRate        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1        for: 5m        labels:          severity: critical        annotations:          summary: "High error rate detected"          description: "Error rate is {{ \$value }} errors per second"                - alert: HighResponseTime        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1        for: 5m        labels:          severity: warning        annotations:          summary: "High response time detected"          description: "95th percentile response time is {{ \$value }} seconds"                - alert: LowDatabaseConnections        expr: pg_stat_database_numbackends < 5        for: 5m        labels:          severity: warning        annotations:          summary: "Low database connections"          description: "Database connections are low: {{ \$value }}"                - alert: HighMemoryUsage        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80        for: 5m        labels:          severity: critical        annotations:          summary: "High memory usage"          description: "Memory usage is at {{ \$value }}%"                - alert: HighCPUUsage        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80        for: 5m        labels:          severity: warning        annotations:          summary: "High CPU usage"          description: "CPU usage is at {{ \$value }}%"                - alert: DiskSpaceLow        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85        for: 5m        labels:          severity: critical        annotations:          summary: "Low disk space"          description: "Disk space usage is at {{ \$value }}%"                - alert: ServiceDown        expr: up == 0        for: 1m        labels:          severity: critical        annotations:          summary: "Service is down"          description: "Service {{ \$labels.instance }} is down"                - alert: HighLatency        expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m]) > 2        for: 5m        labels:          severity: warning        annotations:          summary: "High latency detected"          description: "Average latency is {{ \$value }} seconds"EOF        # Create Grafana dashboard    cat > "monitoring/dashboards/normaldance-dashboard.json" << EOF{  "dashboard": {    "id": null,    "title": "NormalDance Platform Overview",    "tags": ["normaldance", "platform"],    "timezone": "browser",    "panels": [      {        "id": 1,        "title": "HTTP Requests",        "type": "graph",        "targets": [          {            "expr": "rate(http_requests_total[5m])",            "legendFormat": "{{method}} {{status}}"          }        ],        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}      },      {        "id": 2,        "title": "Response Time",        "type": "graph",        "targets": [          {            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",            "legendFormat": "95th percentile"          }        ],        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}      },      {        "id": 3,        "title": "Database Connections",        "type": "singlestat",        "targets": [          {            "expr": "pg_stat_database_numbackends",            "legendFormat": "Connections"          }        ],        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 8}      },      {        "id": 4,        "title": "Memory Usage",        "type": "singlestat",        "targets": [          {            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",            "legendFormat": "Memory Usage"          }        ],        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 8}      },      {        "id": 5,        "title": "CPU Usage",        "type": "singlestat",        "targets": [          {            "expr": "100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",            "legendFormat": "CPU Usage"          }        ],        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 8}      },      {        "id": 6,        "title": "Active Users",        "type": "graph",        "targets": [          {            "expr": "rate(active_users_total[5m])",            "legendFormat": "Active Users"          }        ],        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16}      },      {        "id": 7,        "title": "Track Plays",        "type": "graph",        "targets": [          {            "expr": "rate(track_plays_total[5m])",            "legendFormat": "Track Plays"          }        ],        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16}      }    ],    "time": {"from": "now-1h", "to": "now"},    "timepicker": {      "refresh_intervals": ["5s", "15s", "30s", "1m", "5m", "15m", "30m", "1h", "2h", "6h"]    }  }}EOF        # Create alert manager configuration    cat > "monitoring/alertmanager-config.yaml" << EOFglobal:  smtp_smarthost: 'localhost:587'  smtp_from: 'alerts@normaldance.app'  smtp_auth_username: 'alerts@normaldance.app'  smtp_auth_password: 'your-smtp-password'route:  group_by: ['alertname']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'web.hook'receivers:  - name: 'web.hook'    email_configs:      - to: '$ALERT_EMAILS'        subject: 'NormalDance Alert: {{ .GroupLabels.alertname }}'        body: |          {{ range .Alerts }}          Alert: {{ .Annotations.summary }}          Description: {{ .Annotations.description }}          Status: {{ .Status }}          Labels: {{ .Labels }}          {{ end }}    webhook_configs:      - url: '$SLACK_WEBHOOK'        send_resolved: trueEOF        log "Monitoring infrastructure setup completed"}# Set up health checkssetup_health_checks() {    log "Setting up health checks..."        # Create health check script    cat > "monitoring/scripts/health-check.sh" << EOF#!/bin/bash# NormalDance Health Check Script# This script performs comprehensive health checksset -e# ConfigurationHEALTH_CHECK_DIR="monitoring/health"LOG_DIR="monitoring/logs"API_URL="https://api.normaldance.com"WEB_URL="https://normaldance.app"CHECK_INTERVAL=30MAX_RETRIES=3# Colors for outputGREEN='\033[0;32m'YELLOW='\033[1;33m'RED='\033[0;31m'NC='\033[0m'log() {    echo -e "\${GREEN}[INFO]\${NC} \$1"}warn() {    echo -e "\${YELLOW}[WARN]\${NC} \$1"}error() {    echo -e "\${RED}[ERROR]\${NC} \$1"}# Create health check directorymkdir -p "\$HEALTH_CHECK_DIR"mkdir -p "\$LOG_DIR"# Function to check API healthcheck_api_health() {    local endpoint="\$1"    local max_retries=\$2    local retry_count=0        while [ \$retry_count -lt \$max_retries ]; do        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$API_URL/\$endpoint")                if [ "\$response" -eq 200 ]; then            log "API endpoint \$endpoint is healthy"            return 0        else            warn "API endpoint \$endpoint returned \$response (attempt \$((retry_count + 1))/\$max_retries)"            retry_count=\$((retry_count + 1))            sleep 5        fi    done        error "API endpoint \$endpoint is unhealthy after \$max_retries attempts"    return 1}# Function to check web application healthcheck_web_health() {    local max_retries=\$1    local retry_count=0        while [ \$retry_count -lt \$max_retries ]; do        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$WEB_URL")                if [ "\$response" -eq 200 ]; then            log "Web application is healthy"            return 0        else            warn "Web application returned \$response (attempt \$((retry_count + 1))/\$max_retries)"            retry_count=\$((retry_count + 1))            sleep 5        fi    done        error "Web application is unhealthy after \$max_retries attempts"    return 1}# Function to check database healthcheck_database_health() {    local max_retries=\$1    local retry_count=0        while [ \$retry_count -lt \$max_retries ]; do        local response=\$(curl -s "\$API_URL/health/database" | jq -r '.status')                if [ "\$response" = "healthy" ]; then            log "Database is healthy"            return 0        else            warn "Database is \$response (attempt \$((retry_count + 1))/\$max_retries)"            retry_count=\$((retry_count + 1))            sleep 5        fi    done        error "Database is unhealthy after \$max_retries attempts"    return 1}# Function to check external servicescheck_external_services() {    local services=("https://api.spotify.com" "https://api.apple.com" "https://ipfs.io")        for service in "\${services[@]}"; do        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$service")                if [ "\$response" -eq 200 ]; then            log "External service \$service is accessible"        else            warn "External service \$service returned \$response"        fi    done}# Function to check system resourcescheck_system_resources() {    local cpu_usage=\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)    local memory_usage=\$(free | grep Mem | awk '{printf "%.2f", \$3/\$2 * 100.0}')    local disk_usage=\$(df -h / | awk 'NR==2{print \$5}' | cut -d'%' -f1)        log "System Resources:"    log "  CPU Usage: \$cpu_usage%"    log "  Memory Usage: \$memory_usage%"    log "  Disk Usage: \$disk_usage%"        # Check if resources are within acceptable limits    if (( \$(echo "\$cpu_usage > 80" | bc -l) )); then        warn "High CPU usage detected: \$cpu_usage%"    fi        if (( \$(echo "\$memory_usage > 80" | bc -l) )); then        warn "High memory usage detected: \$memory_usage%"    fi        if [ "\$disk_usage" -gt 80 ]; then        warn "High disk usage detected: \$disk_usage%"    fi}# Function to generate health reportgenerate_health_report() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local report_file="\$HEALTH_CHECK_DIR/health-report-\$timestamp.json"        cat > "\$report_file" << JSON{  "timestamp": "\$(date -Iseconds)",  "api_health": {    "status": "\$(curl -s "\$API_URL/health" | jq -r '.status')",    "uptime": "\$(curl -s "\$API_URL/health" | jq -r '.uptime')",    "version": "\$(curl -s "\$API_URL/health" | jq -r '.version')"  },  "database_health": {    "status": "\$(curl -s "\$API_URL/health/database" | jq -r '.status')",    "connections": "\$(curl -s "\$API_URL/health/database" | jq -r '.connections')",    "queries_per_second": "\$(curl -s "\$API_URL/health/database" | jq -r '.queries_per_second')"  },  "system_resources": {    "cpu_usage": "\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)",    "memory_usage": "\$(free | grep Mem | awk '{printf "%.2f", \$3/\$2 * 100.0}')",    "disk_usage": "\$(df -h / | awk 'NR==2{print \$5}' | cut -d'%' -f1)"  },  "external_services": {    "spotify": "\$(curl -s -o /dev/null -w "%{http_code}" "https://api.spotify.com")",    "apple": "\$(curl -s -o /dev/null -w "%{http_code}" "https://api.apple.com")",    "ipfs": "\$(curl -s -o /dev/null -w "%{http_code}" "https://ipfs.io")"  }}JSON        log "Health report generated: \$report_file"}# Main health check functionperform_health_checks() {    log "Starting comprehensive health checks..."        # Check API health    check_api_health "health" \$MAX_RETRIES    check_api_health "tracks" \$MAX_RETRIES    check_api_health "users" \$MAX_RETRIES    check_api_health "wallet" \$MAX_RETRIES        # Check web application health    check_web_health \$MAX_RETRIES        # Check database health    check_database_health \$MAX_RETRIES        # Check external services    check_external_services        # Check system resources    check_system_resources        # Generate health report    generate_health_report        log "Health checks completed successfully"}# Continuous monitoring functioncontinuous_monitoring() {    log "Starting continuous monitoring..."        while true; do        perform_health_checks                # Log to file        echo "\$(date -Iseconds) - Health check completed" >> "\$LOG_DIR/health-check.log"                # Wait for next check        sleep \$CHECK_INTERVAL    done}# Main functioncase \$1 in    "check")        perform_health_checks        ;;    "monitor")        continuous_monitoring        ;;    *)        echo "Usage: \$0 [check|monitor]"        exit 1        ;;esacEOF        # Make the script executable    chmod +x "monitoring/scripts/health-check.sh"        log "Health checks setup completed"}# Set up analytics trackingsetup_analytics_tracking() {    log "Setting up analytics tracking..."        # Create analytics configuration    cat > "monitoring/analytics-config.json" << EOF{  "google_analytics": {    "tracking_id": "UA-XXXXXXXXX-X",    "property_name": "NormalDance Production",    "custom_dimensions": {      "user_type": "dimension1",      "subscription_plan": "dimension2",      "artist_status": "dimension3"    },    "goals": [      {        "name": "Signups",        "type": "destination",        "match_type": "exact",        "url": "/signup"      },      {        "name": "Track Uploads",        "type": "destination",        "match_type": "exact",        "url": "/upload"      },      {        "name": "Wallet Connections",        "type": "destination",        "match_type": "exact",        "url": "/wallet"      }    ]  },  "mixpanel": {    "token": "your_mixpanel_token",    "events": [      {        "name": "User Signup",        "properties": {          "plan": "free",          "source": "organic"        }      },      {        "name": "Track Upload",        "properties": {          "duration": 180,          "genre": "electronic"        }      },      {        "name": "Wallet Connection",        "properties": {          "wallet_type": "phantom",          "chain": "solana"        }      }    ]  },  "amplitude": {    "api_key": "your_amplitude_api_key",    "events": [      {        "name": "Session Start",        "properties": {          "user_id": "{{user_id}}",          "device_type": "{{device_type}}"        }      },      {        "name": "Track Play",        "properties": {          "track_id": "{{track_id}}",          "artist_id": "{{artist_id}}",          "duration": "{{duration}}"        }      }    ]  }}EOF        # Create analytics tracking script    cat > "monitoring/scripts/analytics-tracking.sh" << EOF#!/bin/bash# NormalDance Analytics Tracking Script# This script handles analytics tracking and reportingset -e# ConfigurationANALYTICS_DIR="monitoring/analytics"ANALYTICS_CONFIG="\$ANALYTICS_DIR/analytics-config.json"LOG_DIR="monitoring/logs"REPORT_DIR="monitoring/reports"# Colors for outputGREEN='\033[0;32m'YELLOW='\033[1;33m'RED='\033[0;31m'NC='\033[0m'log() {    echo -e "\${GREEN}[INFO]\${NC} \$1"}warn() {    echo -e "\${YELLOW}[WARN]\${NC} \$1"}error() {    echo -e "\${RED}[ERROR]\${NC} \$1"}# Create directoriesmkdir -p "\$LOG_DIR"mkdir -p "\$REPORT_DIR"# Function to track user signuptrack_user_signup() {    local user_id=\$1    local email=\$2    local plan=\$3    local source=\$4        log "Tracking user signup for \$user_id"        # Track in Google Analytics    curl -X POST "https://www.google-analytics.com/mp/collect" \\        -H "Content-Type: application/json" \\        -H "Authorization: Bearer \$GA_API_SECRET" \\        -d "{            \"client_id\": \"\$user_id\",            \"events\": [{                \"name\": \"sign_up\",                \"params\": {                    \"email\": \"\$email\",                    \"plan\": \"\$plan\",                    \"source\": \"\$source\"                }            }]        }"        # Track in Mixpanel    curl -X POST "https://api.mixpanel.com/track" \\        -H "Content-Type: application/json" \\        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\        -d "{            \"event\": \"User Signup\",            \"properties\": {                \"distinct_id\": \"\$user_id\",                \"\$email\": \"\$email\",                \"plan\": \"\$plan\",                \"source\": \"\$source\"            }        }"        # Track in Amplitude    curl -X POST "https://api2.amplitude.com/2/httpapi" \\        -H "Content-Type: application/json" \\        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\        -d "{            \"api_key\": \"\$AMPLITUDE_API_KEY\",            \"events\": [{                \"user_id\": \"\$user_id\",                \"event_type\": \"User Signup\",                \"event_properties\": {                    \"email\": \"\$email\",                    \"plan\": \"\$plan\",                    \"source\": \"\$source\"                }            }]        }"}# Function to track track playtrack_track_play() {    local user_id=\$1    local track_id=\$2    local artist_id=\$3    local duration=\$4        log "Tracking track play for \$track_id by \$user_id"        # Track in Google Analytics    curl -X POST "https://www.google-analytics.com/mp/collect" \\        -H "Content-Type: application/json" \\        -H "Authorization: Bearer \$GA_API_SECRET" \\        -d "{            \"client_id\": \"\$user_id\",            \"events\": [{                \"name\": \"track_play\",                \"params\": {                    \"track_id\": \"\$track_id\",                    \"artist_id\": \"\$artist_id\",                    \"duration\": \"\$duration\"                }            }]        }"        # Track in Mixpanel    curl -X POST "https://api.mixpanel.com/track" \\        -H "Content-Type: application/json" \\        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\        -d "{            \"event\": \"Track Play\",            \"properties\": {                \"distinct_id\": \"\$user_id\",                \"track_id\": \"\$track_id\",                \"artist_id\": \"\$artist_id\",                \"duration\": \"\$duration\"            }        }"        # Track in Amplitude    curl -X POST "https://api2.amplitude.com/2/httpapi" \\        -H "Content-Type: application/json" \\        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\        -d "{            \"api_key\": \"\$AMPLITUDE_API_KEY\",            \"events\": [{                \"user_id\": \"\$user_id\",                \"event_type\": \"Track Play\",                \"event_properties\": {                    \"track_id\": \"\$track_id\",                    \"artist_id\": \"\$artist_id\",                    \"duration\": \"\$duration\"                }            }]        }"}# Function to generate analytics reportgenerate_analytics_report() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local report_file="\$REPORT_DIR/analytics-report-\$timestamp.json"        # Get analytics data from various sources    local ga_data=\$(curl -s "https://www.google-analytics.com/data" \\        -H "Authorization: Bearer \$GA_API_SECRET" \\        -d "{            \"metrics\": [                {                    \"name\": \"activeUsers\",                    \"expression\": \"activeUsers\"                }            ],            \"dimensions\": [                {                    \"name\": \"date\"                }            ]        }")        local mixpanel_data=\$(curl -s "https://api.mixpanel.com/export" \\        -H "Authorization: Basic \$MIXPANEL_API_KEY" \\        -d "{            \"event\": \"[\\\"User Signup\\\", \\\"Track Play\\\"]\",            \"from_date\": \"2024-01-01\",            \"to_date\": \"2024-12-31\"        }")        local amplitude_data=\$(curl -s "https://api.amplitude.com/api/2/retention" \\        -H "Content-Type: application/json" \\        -H "Authorization: Basic \$AMPLITUDE_API_KEY" \\        -d "{            \"group_type\": \"user\",            \"group_values\": [\"all\"],            \"period\": \"day\",            \"start\": \"2024-01-01\",            \"end\": \"2024-12-31\"        }")        # Combine data into report    cat > "\$report_file" << JSON{  "timestamp": "\$(date -Iseconds)",  "google_analytics": \$ga_data,  "mixpanel": \$mixpanel_data,  "amplitude": \$amplitude_data,  "summary": {    "total_users": \$(curl -s "\$API_URL/users/count" | jq -r '.count'),    "active_users_7d": \$(curl -s "\$API_URL/users/active?period=7d" | jq -r '.count'),    "total_tracks": \$(curl -s "\$API_URL/tracks/count" | jq -r '.count'),    "total_plays": \$(curl -s "\$API_URL/tracks/plays" | jq -r '.count'),    "total_revenue": \$(curl -s "\$API_URL/revenue/total" | jq -r '.amount')  }}JSON        log "Analytics report generated: \$report_file"}# Main functioncase \$1 in    "track-signup")        track_user_signup "\$2" "\$3" "\$4" "\$5"        ;;    "track-play")        track_track_play "\$2" "\$3" "\$4" "\$5"        ;;    "report")        generate_analytics_report        ;;    *)        echo "Usage: \$0 [track-signup|track-play|report] [args...]"        exit 1        ;;esacEOF        # Make the script executable    chmod +x "monitoring/scripts/analytics-tracking.sh"        log "Analytics tracking setup completed"}# Set up performance monitoringsetup_performance_monitoring() {    log "Setting up performance monitoring..."        # Create performance monitoring script    cat > "monitoring/scripts/performance-monitoring.sh" << EOF#!/bin/bash# NormalDance Performance Monitoring Script# This script monitors application performanceset -e# ConfigurationPERFORMANCE_DIR="monitoring/performance"LOG_DIR="monitoring/logs"API_URL="https://api.normaldance.com"CHECK_INTERVAL=60MAX_RESPONSE_TIME=2000MAX_ERROR_RATE=0.01# Colors for outputGREEN='\033[0;32m'YELLOW='\033[1;33m'RED='\033[0;31m'NC='\033[0m'log() {    echo -e "\${GREEN}[INFO]\${NC} \$1"}warn() {    echo -e "\${YELLOW}[WARN]\${NC} \$1"}error() {    echo -e "\${RED}[ERROR]\${NC} \$1"}# Create directoriesmkdir -p "\$PERFORMANCE_DIR"mkdir -p "\$LOG_DIR"# Function to measure API response timemeasure_api_response_time() {    local endpoint=\$1    local iterations=\${2:-10}    local total_time=0        for ((i=1; i<=\$iterations; i++)); do        local start_time=\$(date +%s%N)        curl -s "\$API_URL/\$endpoint" > /dev/null        local end_time=\$(date +%s%N)        local duration=\$((end_time - start_time))        total_time=\$((total_time + duration))    done        local average_time=\$((total_time / iterations))    echo "\$average_time"}# Function to measure error ratemeasure_error_rate() {    local endpoint=\$1    local iterations=\${2:-100}    local error_count=0        for ((i=1; i<=\$iterations; i++)); do        local response=\$(curl -s -o /dev/null -w "%{http_code}" "\$API_URL/\$endpoint")                if [ "\$response" -ge 400 ]; then            error_count=\$((error_count + 1))        fi    done        local error_rate=\$(echo "scale=4; \$error_count / \$iterations" | bc)    echo "\$error_rate"}# Function to measure database performancemeasure_database_performance() {    local query_time=\$(curl -s "\$API_URL/health/database" | jq -r '.query_time')    local connection_count=\$(curl -s "\$API_URL/health/database" | jq -r '.connection_count')        echo "Query Time: \$query_time ms, Connections: \$connection_count"}# Function to measure memory usagemeasure_memory_usage() {    local memory_usage=\$(ps -o pid,ppid,cmd,%mem,%cpu --sort=-%mem | head -6)    echo "\$memory_usage"}# Function to measure CPU usagemeasure_cpu_usage() {    local cpu_usage=\$(top -bn1 | grep "Cpu(s)" | awk '{print \$2}' | cut -d'%' -f1)    echo "\$cpu_usage"}# Function to generate performance reportgenerate_performance_report() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local report_file="\$PERFORMANCE_DIR/performance-report-\$timestamp.json"        # Measure various performance metrics    local api_response_time=\$(measure_api_response_time "health" 10)    local error_rate=\$(measure_error_rate "health" 100)    local db_performance=\$(measure_database_performance)    local memory_usage=\$(measure_memory_usage)    local cpu_usage=\$(measure_cpu_usage)        # Create performance report    cat > "\$report_file" << JSON{  "timestamp": "\$(date -Iseconds)",  "api_performance": {    "response_time_ms": \$api_response_time,    "error_rate": \$error_rate,    "status": "\$([ \"\$api_response_time\" -lt \"\$MAX_RESPONSE_TIME\" ] && echo \"healthy\" || echo \"degraded\")"  },  "database_performance": {    "performance": "\$db_performance",    "status": "\$([ \"\$db_performance\" =~ \"Query Time: [0-9]*\" ] && echo \"healthy\" || echo \"degraded\")"  },  "system_resources": {    "memory_usage": "\$memory_usage",    "cpu_usage": "\$cpu_usage",    "status": "\$([ \"\$cpu_usage\" -lt \"80\" ] && echo \"healthy\" || echo \"degraded\")"  },  "recommendations": [    "\$([ \"\$api_response_time\" -ge \"\$MAX_RESPONSE_TIME\" ] && echo \"Consider optimizing API endpoints\" || echo \"API performance is good\")",    "\$([ \"\$error_rate\" -ge \"\$MAX_ERROR_RATE\" ] && echo \"Error rate is high, investigate API errors\" || echo \"Error rate is within acceptable limits\")",    "\$([ \"\$cpu_usage\" -ge \"80\" ] && echo \"High CPU usage detected, consider scaling\" || echo \"CPU usage is normal\")"  ]}JSON        log "Performance report generated: \$report_file"}# Function to check performance thresholdscheck_performance_thresholds() {    local api_response_time=\$(measure_api_response_time "health" 10)    local error_rate=\$(measure_error_rate "health" 100)    local cpu_usage=\$(measure_cpu_usage)        # Check API response time    if [ "\$api_response_time" -ge "\$MAX_RESPONSE_TIME" ]; then        warn "API response time is high: \$api_response_time ms"    else        log "API response time is good: \$api_response_time ms"    fi        # Check error rate    if (( \$(echo "\$error_rate >= \$MAX_ERROR_RATE" | bc -l) )); then        warn "Error rate is high: \$error_rate"    else        log "Error rate is good: \$error_rate"    fi        # Check CPU usage    if [ "\$cpu_usage" -ge 80 ]; then        warn "CPU usage is high: \$cpu_usage%"    else        log "CPU usage is good: \$cpu_usage%"    fi}# Main functioncase \$1 in    "measure")        generate_performance_report        ;;    "check")        check_performance_thresholds        ;;    "monitor")        while true; do            check_performance_thresholds            sleep \$CHECK_INTERVAL        done        ;;    *)        echo "Usage: \$0 [measure|check|monitor]"        exit 1        ;;esacEOF        # Make the script executable    chmod +x "monitoring/scripts/performance-monitoring.sh"        log "Performance monitoring setup completed"}# Set up log aggregationsetup_log_aggregation() {    log "Setting up log aggregation..."        # Create log aggregation configuration    cat > "monitoring/log-aggregation-config.yaml" << EOFversion: '3.8'services:  fluentd:    image: fluent/fluentd:v1.16-1    volumes:      - ./fluentd/conf:/fluentd/etc    ports:      - "24224:24224"      - "24224:24224/udp"    environment:      - FLUENTD_CONF=fluent.conf  elasticsearch:    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0    environment:      - discovery.type=single-node      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"    ports:      - "9200:9200"    volumes:      - elasticsearch_data:/usr/share/elasticsearch/data  kibana:    image: docker.elastic.co/kibana/kibana:8.11.0    ports:      - "5601:5601"    environment:      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    depends_on:      - elasticsearchvolumes:  elasticsearch_data:EOF        # Create Fluentd configuration    cat > "monitoring/fluentd/fluent.conf" << EOF<source>  @type forward  port 24224  bind 0.0.0.0</source><match normaldance.**>  @type elasticsearch  host elasticsearch  port 9200  index_name normaldance-logs  type_name _doc  include_tag_key true  tag_key @log_name  <buffer>    flush_mode interval    flush_interval 5s    flush_thread_count 8    chunk_limit_size 2m    queue_limit_length 32    retry_max_interval 30  </buffer></match><match **>  @type stdout</match>EOF        # Create log collection script    cat > "monitoring/scripts/log-collection.sh" << EOF#!/bin/bash# NormalDance Log Collection Script# This script collects and aggregates logsset -e# ConfigurationLOG_DIR="monitoring/logs"AGGREGATION_DIR="monitoring/aggregation"DOCKER_COMPOSE_FILE="monitoring/log-aggregation-config.yaml"# Colors for outputGREEN='\033[0;32m'YELLOW='\033[1;33m'RED='\033[0;31m'NC='\033[0m'log() {    echo -e "\${GREEN}[INFO]\${NC} \$1"}warn() {    echo -e "\${YELLOW}[WARN]\${NC} \$1"}error() {    echo -e "\${RED}[ERROR]\${NC} \$1"}# Create directoriesmkdir -p "\$LOG_DIR"mkdir -p "\$AGGREGATION_DIR"# Function to start log aggregationstart_log_aggregation() {    log "Starting log aggregation..."        # Start Docker containers    docker-compose -f "\$DOCKER_COMPOSE_FILE" up -d        log "Log aggregation started"}# Function to stop log aggregationstop_log_aggregation() {    log "Stopping log aggregation..."        # Stop Docker containers    docker-compose -f "\$DOCKER_COMPOSE_FILE" down        log "Log aggregation stopped"}# Function to collect logscollect_logs() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local log_file="\$LOG_DIR/application-\$timestamp.log"        # Collect application logs    docker logs normaldance-api > "\$log_file" 2>&1        # Collect system logs    journalctl -u normaldance-api > "\$LOG_DIR/system-\$timestamp.log" 2>&1        # Collect access logs    tail -n 1000 /var/log/nginx/access.log > "\$LOG_DIR/access-\$timestamp.log" 2>&1        log "Logs collected to \$log_file"}# Function to search logssearch_logs() {    local query=\$1    local log_type=\${2:-"all"}        case \$log_type in        "api")            grep -r "\$query" "\$LOG_DIR"/*api*.log            ;;        "system")            grep -r "\$query" "\$LOG_DIR"/*system*.log            ;;        "access")            grep -r "\$query" "\$LOG_DIR"/*access*.log            ;;        *)            grep -r "\$query" "\$LOG_DIR"/*.log            ;;    esac}# Function to generate log reportgenerate_log_report() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local report_file="\$AGGREGATION_DIR/log-report-\$timestamp.json"        # Count log entries by type    local api_logs=\$(find "\$LOG_DIR" -name "*api*.log" | wc -l)    local system_logs=\$(find "\$LOG_DIR" -name "*system*.log" | wc -l)    local access_logs=\$(find "\$LOG_DIR" -name "*access*.log" | wc -l)        # Count error logs    local error_logs=\$(grep -r "ERROR" "\$LOG_DIR"/*.log | wc -l)    local warning_logs=\$(grep -r "WARN" "\$LOG_DIR"/*.log | wc -l)        # Create log report    cat > "\$report_file" << JSON{  "timestamp": "\$(date -Iseconds)",  "log_counts": {    "api_logs": \$api_logs,    "system_logs": \$system_logs,    "access_logs": \$access_logs  },  "error_counts": {    "errors": \$error_logs,    "warnings": \$warning_logs  },  "log_sizes": {    "total_size": \$(du -sh "\$LOG_DIR" | cut -f1),    "api_size": \$(du -sh "\$LOG_DIR"/*api*.log 2>/dev/null | cut -f1 || echo "0"),    "system_size": \$(du -sh "\$LOG_DIR"/*system*.log 2>/dev/null | cut -f1 || echo "0"),    "access_size": \$(du -sh "\$LOG_DIR"/*access*.log 2>/dev/null | cut -f1 || echo "0")  },  "recent_errors": [    \$(grep -r "ERROR" "\$LOG_DIR"/*.log | tail -5 | jq -R -s -c '.')  ]}JSON        log "Log report generated: \$report_file"}# Main functioncase \$1 in    "start")        start_log_aggregation        ;;    "stop")        stop_log_aggregation        ;;    "collect")        collect_logs        ;;    "search")        search_logs "\$2" "\$3"        ;;    "report")        generate_log_report        ;;    *)        echo "Usage: \$0 [start|stop|collect|search|report] [args...]"        exit 1        ;;esacEOF        # Make the script executable    chmod +x "monitoring/scripts/log-collection.sh"        log "Log aggregation setup completed"}# Set up backup and disaster recoverysetup_backup_disaster_recovery() {    log "Setting up backup and disaster recovery..."        # Create backup configuration    cat > "monitoring/backup-config.yaml" << EOFversion: '3.8'services:  postgres-backup:    image: postgres:15    volumes:      - postgres_data:/var/lib/postgresql/data      - ./backups:/backups    environment:      - POSTGRES_DB=normaldance      - POSTGRES_USER=normaldance      - POSTGRES_PASSWORD=\${POSTGRES_PASSWORD}    command: >      bash -c "        while true; do          pg_dump -U normaldance normaldance > /backups/postgres-\$(date +%Y%m%d_%H%M%S).sql          sleep 86400        done      "  redis-backup:    image: redis:7    volumes:      - redis_data:/data      - ./backups:/backups    command: >      bash -c "        while true; do          redis-cli --rdb /backups/redis-\$(date +%Y%m%d_%H%M%S).rdb          sleep 86400        done      "  file-backup:    image: alpine:latest    volumes:      - ./backups:/backups      - ./uploads:/uploads    command: >      bash -c "        while true; do          tar -czf /backups/files-\$(date +%Y%m%d_%H%M%S).tar.gz -C /uploads .          sleep 86400        done      "volumes:  postgres_data:  redis_data:EOF        # Create backup script    cat > "monitoring/scripts/backup.sh" << EOF#!/bin/bash# NormalDance Backup Script# This script handles database and file backupsset -e# ConfigurationBACKUP_DIR="monitoring/backups"RETENTION_DAYS=30AWS_S3_BUCKET="s3://normaldance-backups"AWS_ACCESS_KEY_ID="\${AWS_ACCESS_KEY_ID}"AWS_SECRET_ACCESS_KEY="\${AWS_SECRET_ACCESS_KEY}"POSTGRES_HOST="\${POSTGRES_HOST:-localhost}"POSTGRES_USER="\${POSTGRES_USER:-normaldance}"POSTGRES_PASSWORD="\${POSTGRES_PASSWORD}"POSTGRES_DB="\${POSTGRES_DB:-normaldance}"REDIS_HOST="\${REDIS_HOST:-localhost}"UPLOADS_DIR="\${UPLOADS_DIR:-./uploads}"# Colors for outputGREEN='\033[0;32m'YELLOW='\033[1;33m'RED='\033[0;31m'NC='\033[0m'log() {    echo -e "\${GREEN}[INFO]\${NC} \$1"}warn() {    echo -e "\${YELLOW}[WARN]\${NC} \$1"}error() {    echo -e "\${RED}[ERROR]\${NC} \$1"}# Create backup directorymkdir -p "\$BACKUP_DIR"# Function to backup PostgreSQLbackup_postgres() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local backup_file="\$BACKUP_DIR/postgres-\$timestamp.sql"        log "Starting PostgreSQL backup..."        # Create backup    PGPASSWORD="\$POSTGRES_PASSWORD" pg_dump -h "\$POSTGRES_HOST" -U "\$POSTGRES_USER" "\$POSTGRES_DB" > "\$backup_file"        # Compress backup    gzip "\$backup_file"        # Upload to S3    aws s3 cp "\$backup_file.gz" "\$AWS_S3_BUCKET/postgres/" --acl bucket-owner-full-control        log "PostgreSQL backup completed: \$backup_file.gz"}# Function to backup Redisbackup_redis() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local backup_file="\$BACKUP_DIR/redis-\$timestamp.rdb"        log "Starting Redis backup..."        # Create backup    redis-cli -h "\$REDIS_HOST" BGSAVE        # Copy backup file    cp "/var/lib/redis/dump.rdb" "\$backup_file"        # Compress backup    gzip "\$backup_file"        # Upload to S3    aws s3 cp "\$backup_file.gz" "\$AWS_S3_BUCKET/redis/" --acl bucket-owner-full-control        log "Redis backup completed: \$backup_file.gz"}# Function to backup filesbackup_files() {    local timestamp=\$(date +%Y%m%d_%H%M%S)    local backup_file="\$BACKUP_DIR/files-\$timestamp.tar.gz"        log "Starting files backup..."        # Create backup    tar -czf "\$backup_file" -C "\$UPLOADS_DIR" .        # Upload to S3    aws s3 cp "\$backup_file" "\$AWS_S3_BUCKET/files/" --acl bucket-owner-full-control        log "Files backup completed: \$backup_file"}# Function to cleanup old backupscleanup_old_backups() {    log "Cleaning up old backups..."        # Find and remove backups older than retention period    find "\$BACKUP_DIR" -name "*.sql.gz" -mtime +\$RETENTION_DAYS -delete    find "\$BACKUP_DIR" -name "*.rdb.gz" -mtime +\$RETENTION_DAYS -delete    find "\$BACKUP_DIR" -name "*.tar.gz" -mtime +\$RETENTION_DAYS -delete        # Cleanup S3    aws s3 ls "\$AWS_S3_BUCKET/" --recursive | while read -r line; do        createDate=\$(echo \$line | awk '{print $1" "$2}')        createDateTS=\$(date -d "\$createDate" +%s)        olderThanTS=\$(date -d "\$RETENTION_DAYS days ago" +%s)                if [ \$createDateTS -lt \$olderThanTS ]; then            fileName=\$(echo \$line | awk '{print $4}')            aws s3 rm "\$AWS_S3_BUCKET/\$fileName"        fi    done        log "Cleanup completed"}# Function to restore from backuprestore_backup() {    local backup_type=\$1    local backup_file=\$2        if [ ! -f "\$backup_file" ]; then        error "Backup file not found: \$backup_file"    fi        case \$backup_type in        "postgres")            log "Restoring PostgreSQL from \$backup_file"            gunzip -c "\$backup_file" | psql -h "\$POSTGRES_HOST" -U "\$POSTGRES_USER" "\$POSTGRES_DB"            ;;        "redis")            log "Restoring Redis from \$backup_file"            gunzip -c "\$backup_file" > /var/lib/redis/dump.rdb            redis-cli -h "\$REDIS_HOST" BGSAVE            ;;        "files")            log "Restoring files from \$backup_file"            tar -xzf "\$backup_file" -C "\$UPLOADS_DIR"            ;;        *)            error "Unknown backup type: \$backup_type"            ;;    esac        log "Restore completed"}# Main functioncase \$1 in    "postgres")        backup_postgres        ;;    "redis")        backup_redis        ;;    "files")        backup_files        ;;    "cleanup")        cleanup_old_backups        ;;    "restore")        restore_backup "\$2" "\$3"        ;;    "all")        backup_postgres        backup_redis        backup_files        cleanup_old_backups        ;;    *)        echo "Usage: \$0 [postgres|redis|files|cleanup|restore|all] [backup_file]"        exit 1        ;;esacEOF        # Make the script executable    chmod +x "monitoring/scripts/backup.sh"        log "Backup and disaster recovery setup completed"}# Main functionmain() {    log "Starting NormalDance launch and monitoring setup..."        # Check prerequisites    check_prerequisites        # Create monitoring directory    create_monitoring_directory        # Set up monitoring infrastructure    setup_monitoring_infrastructure        # Set up health checks    setup_health_checks        # Set up analytics tracking    setup_analytics_tracking        # Set up performance monitoring    setup_performance_monitoring        # Set up log aggregation    setup_log_aggregation        # Set up backup and disaster recovery    setup_backup_disaster_recovery        log "NormalDance launch and monitoring setup completed successfully!"        # Display next steps    echo ""    echo "Next Steps:"    echo "1. Configure monitoring tools with your credentials"    echo "2. Start monitoring services: ./monitoring/scripts/health-check.sh monitor"    echo "3. Set up alerts and notifications"    echo "4. Configure backup automation"    echo "5. Test disaster recovery procedures"    echo ""    echo "Monitoring Dashboard: \$GRAFANA_URL"    echo "Alert Manager: \$SLACK_WEBHOOK"    echo "Backup Location: \$AWS_S3_BUCKET"}# Run main functionmain "$@"